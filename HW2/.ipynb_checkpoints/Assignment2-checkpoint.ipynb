{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 (100 points)\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots and texts. You might need to create new cells with original code or text for your analyses. This assignment has a total of 100 points.\n",
    "\n",
    "For assignment submission, you will submit this notebook file (.ipynb) on Canvas with cells executed and outputs visible. Your submitted notebook **must** follow these guidelines:\n",
    "- No other dataset than the provided datasets should be used.\n",
    "- Training, validation and testing splits should be the same as the ones provided.\n",
    "- The cell outputs in your delivered notebook should be reproducible.\n",
    "- Printing out the evaluation metric evidence that your model achieves the evaluation requirement. Optionally, you can also add plot of the evaluation metric changing over the course of training process.\n",
    "- Providing code associated with the conclusions you make in your analysis as well as code that is used to generated plot, images, etc. for your analysis.\n",
    "- All code must be your own work. Code cannot be copied from external sources or another students. You may copy code from cells that are pre-defined in this notebook if you think it is useful to reuse in another question.\n",
    "- All images must be generated from data generated in your code. Do **NOT** import/display images that are generated outside your code.\n",
    "- Your analysis must be your own, but if you quote text or equations from another source please make sure to cite the appropriate references.\n",
    "- Your input with code will be marked with comments ``###your code starts here###`` and ``###your code ends here###`` to specify where you need to write your code. You can also create a new code cell in between those marked comments.\n",
    "\n",
    "\n",
    "**NOTES:**\n",
    "- PyTorch needs to be downloaded and installed properly.\n",
    "- You should use PyTorch 1.7 or later.\n",
    "- If you need to import a different package than the ones already imported, please check with the TA if you can do so.\n",
    "- Cells should be run in order, using Shift+Enter.\n",
    "- Read all the provided code cells and comments as they contain variables and information that you may need to use to complete the notebook.\n",
    "- To create a new text cell, select \"+\" button on the menu bar and change its type from \"Code\" to \"Markdown\".\n",
    "- To modify a text cell, double click on it.\n",
    "- More details on how to format markdown text can be found here: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "- Your home directory on CADE machines has a small disk quota. It might be necessary, depending on how much your home directory is already occupied, to store the virtual environment inside a folder in ```/scratch/tmp/```. \n",
    "- **The accuracy requirement for each question is there to make sure you have performed sufficient amount of experiments to achieve a good result. Part of the grade is based on this.**\n",
    "\n",
    "**Tips for training deep learning models:**\n",
    "- Since the datasets being used here are small, you are probably going to have to use early stopping to prevent overfitting. This means that you will have to save your models in the middle of training. One of the ways to do so is to make a deep copy of it using ```copy.deepcopy``` function. \n",
    "- It is recommended to frequently monitor the behavior of the model at least once every epoch. You can either print out the training loss or evaluation metric of the training set to verify that the model is being optimized correctly. In this assignment, it should take somewhere between 1 and 20 epochs for a desired model to achieve the required accuracy.\n",
    "- To search for the best hyperparameters for your model, it is usually better to start searching in a logarithmic scale. Usually power of 2 or 10 is used. \n",
    "- In https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2, an ablation study in finding optimal learning rates for different optimizers are listed. This could help in your search of optimal learning rate. For this assignment, you will probably get the best results with Adam optimizer and searching for the best learning rate in the range of learning rates provided in that article, which is from 0.00005 to 0.01 unless a question is asked to use a specific optimizer.\n",
    "- Batch size seems to have a smaller impact than learning rate in the results. It should be enough if you test batch sizes between 8 and 32.\n",
    "- We assume a GPU of at least 4GB of memory is available. If you want to try running the assignment with a GPU that has less than that, you can try changing the argument passed when calling the ```define_gpu_to_use``` function.  If you are getting out-of-memory errors for the GPU, you may want to check what is occupying the GPU memory by using the command ```!nvidia-smi```, which gives a usage report of the GPU. However, if you are using your own Windows machine, the nvidia-smi command used in the define_gpu_to_use function will not work. You can skip running this function but please check to make sure your GPU has a sufficient amount of free memory.\n",
    "- For some of the questions, it might be useful for you to understand what the ResNet-18 PyTorch model is doing. You can have access to its source code here (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py). The most important part you should know is the ```forward``` function from the ```ResNet``` class. \n",
    "- It might also be useful to print PyTorch models using ```print(\"model_name\")```. This should give you a list of all the layers in the model.\n",
    "- Here are a few PyTorch details not to forget:\n",
    "    - Toggle train/eval mode for your model\n",
    "    - Reset the gradients with ```zero_grad()``` before each call to ```backward()```\n",
    "    - Check if the loss you are using receives logits or probabilities, and adapt your model output accordingly.\n",
    "    - Reinstantiate your model every time you are starting a new training so that the weights are reset, if you plan to reuse the variable name.\n",
    "    - Pass the model's parameters to the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0 - Set-up Infrastructure (Total of 0 points)\n",
    "## + Install Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydicom\n",
      "  Downloading pydicom-2.1.2-py3-none-any.whl (1.9 MB)\n",
      "Installing collected packages: pydicom\n",
      "Successfully installed pydicom-2.1.2\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jordan\\anaconda3\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\jordan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jordan\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\jordan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jordan\\anaconda3\\lib\\site-packages (from scikit-learn) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q kaggle\n",
    "!pip3 install pydicom\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import copy\n",
    "import torchvision.models as models\n",
    "import tarfile\n",
    "import time\n",
    "from packaging import version\n",
    "%matplotlib inline\n",
    "\n",
    "##### Check Torch library requirement #####\n",
    "my_torch_version = torch.__version__\n",
    "minimum_torch_version = '1.7'\n",
    "if version.parse(my_torch_version) < version.parse(minimum_torch_version):\n",
    "    print('Warning!!! Your Torch version %s does NOT meet the minimum requirement!\\\n",
    "            Please update your Torch library\\n' %my_torch_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Create Data Folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Check what kind of system you are using #####\n",
    "try:\n",
    "    hostname = !hostname\n",
    "    if 'lab' in hostname[0] and '.eng.utah.edu' in hostname[0]:\n",
    "        IN_CADE = True\n",
    "    else:\n",
    "        IN_CADE = False\n",
    "except:\n",
    "    IN_CADE = False\n",
    "\n",
    "## Define the folders where datasets will be\n",
    "machine_being_used = 'cade' if IN_CADE else ('other')\n",
    "pre_folder = '/scratch/tmp/' if machine_being_used == 'cade' else './'\n",
    "mnist_dataset_folder = pre_folder + 'deep_learning_datasets_ECE_6960_013/mnist'\n",
    "xray14_dataset_folder = pre_folder + 'deep_learning_datasets_ECE_6960_013/chestxray14'\n",
    "pneumonia_dataset_folder = pre_folder + 'deep_learning_datasets_ECE_6960_013/kaggle_pneumonia'\n",
    "\n",
    "## Create directory if they haven't existed yet \n",
    "if not os.path.exists(mnist_dataset_folder):\n",
    "    os.makedirs(mnist_dataset_folder)    \n",
    "if machine_being_used != 'cade' and not os.path.exists(mnist_dataset_folder+'/MNIST'):        \n",
    "    os.makedirs(mnist_dataset_folder+'/MNIST')\n",
    "if machine_being_used != 'cade' and not os.path.exists(mnist_dataset_folder+'/MNIST/raw'):\n",
    "    os.makedirs(mnist_dataset_folder+'/MNIST/raw')\n",
    "if not os.path.exists(xray14_dataset_folder):\n",
    "    os.makedirs(xray14_dataset_folder)\n",
    "if not os.path.exists(pneumonia_dataset_folder):\n",
    "    os.makedirs(pneumonia_dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Request GPU Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: \"'nvidia-smi' is not recognized as an internal or external command,\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8be4f9cc5c1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m## Request a gpu and reserve the memory space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mdefine_gpu_to_use\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-8be4f9cc5c1b>\u001b[0m in \u001b[0;36mdefine_gpu_to_use\u001b[1;34m(minimum_memory_mb)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfree_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'No devices were found'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mfree_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfree_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfree_memory\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mminimum_memory_mb\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mthres_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: \"'nvidia-smi' is not recognized as an internal or external command,\""
     ]
    }
   ],
   "source": [
    "##### Request a GPU #####\n",
    "## This function locates an available gpu for usage. In addition, this function reserves a specificed\n",
    "## memory space exclusively for your account. The memory reservation prevents the decrement in computational\n",
    "## speed when other users try to allocate memory on the same gpu in the shared systems, i.e., CADE machines. \n",
    "## Note: If you use your own system which has a GPU with less than 4GB of memory, remember to change the \n",
    "## specified mimimum memory.\n",
    "def define_gpu_to_use(minimum_memory_mb = 3500):    \n",
    "    thres_memory = 600 #\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        \n",
    "        if free_memory>minimum_memory_mb-thres_memory:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "            \n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' + str(minimum_memory_mb) \\\n",
    "              + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-thres_memory)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()        \n",
    "        del x\n",
    "        \n",
    "## Request a gpu and reserve the memory space\n",
    "define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Define Utility Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Preprocess Image #####\n",
    "## This function is used to crop the largest 1:1 aspect ratio region of a given image.\n",
    "## This is useful, especially for medical datasets, since many datasets have images\n",
    "## with different aspect ratios and this is one way to standardize inputs' size.\n",
    "class CropBiggestCenteredInscribedSquare(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        longer_side = min(tensor.size)\n",
    "        horizontal_padding = (longer_side - tensor.size[0]) / 2\n",
    "        vertical_padding = (longer_side - tensor.size[1]) / 2\n",
    "        return tensor.crop(\n",
    "            (\n",
    "                -horizontal_padding,\n",
    "                -vertical_padding,\n",
    "                tensor.size[0] + horizontal_padding,\n",
    "                tensor.size[1] + vertical_padding\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Split a dataset for training, validatation, and testing #####\n",
    "## This function splits a given dataset into 3 subsets of 60%-20%-20% for train-val-test, respectively.\n",
    "## This function is used internally in the dataset classes below.\n",
    "def get_split(array_to_split, split):\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(array_to_split)\n",
    "    np.random.seed()\n",
    "    if split == 'train':\n",
    "        array_to_split = array_to_split[:int(len(array_to_split)*0.6)]\n",
    "    elif split == 'val':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.6):int(len(array_to_split)*0.8)]\n",
    "    elif split == 'test':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.8):]\n",
    "    return array_to_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute the number parameters (weights) #####\n",
    "## This function computes the number of learnable parameters in a Pytorch model\n",
    "def count_number_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - MNIST Dataset and CNNs (Total of 12 points)\n",
    "We begin this assignment by revisiting one of the problems from the previous assignment. Previously, we built a simple two-layer neural network to classify images containing hand-written digits. In this exercise, we are going to replace that two-layer neural network with a convolutional neural network. First, we load the images that we are going to work with into Pytorch Dataloader, and then define an evaluation metric for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load MNIST dataset to Pytorch Dataloader #####\n",
    "## Dowloand MNIST dataset train set from Pytorch (60,000 images total)\n",
    "mnist_training_data = torchvision.datasets.MNIST(mnist_dataset_folder, train = True, \\\n",
    "                                        transform=transforms.ToTensor(), \\\n",
    "                                        target_transform=None, \\\n",
    "                                        download= True)\n",
    "print('A summary of MNIST dataset:\\n')\n",
    "print(mnist_training_data,'\\n')\n",
    "## Dowloand MNIST dataset test set from Pytorch (10,000 images total)\n",
    "mnist_test_data = torchvision.datasets.MNIST(mnist_dataset_folder, train = False, \\\n",
    "                                        transform=transforms.ToTensor(), \\\n",
    "                                        target_transform=None, \\\n",
    "                                        download= True)\n",
    "print(mnist_test_data)\n",
    "## Randomly split training images into 80%/20% for training/validation process\n",
    "train_data_ex1, val_data_ex1 = torch.utils.data.random_split(mnist_training_data, \\\n",
    "                                    [int(0.8*len(mnist_training_data)),\\\n",
    "                                     len(mnist_training_data)-int(0.8*len(mnist_training_data))], \\\n",
    "                                     generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "## Please contact the TA if any of the assertions fails\n",
    "assert(len(mnist_test_data) == 10000)\n",
    "assert(len(train_data_ex1)+len(val_data_ex1) == 60000)\n",
    "\n",
    "## Load files to Pytorch dataloader for training, validation, and testing\n",
    "train_loader_ex1 = torch.utils.data.DataLoader(train_data_ex1, batch_size=16, shuffle=True, num_workers=8)\n",
    "val_loader_ex1 = torch.utils.data.DataLoader(val_data_ex1, batch_size=128, shuffle=True, num_workers=8)\n",
    "test_loader_ex1 = torch.utils.data.DataLoader(mnist_test_data, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute accuracy for MNIST dataset #####\n",
    "def get_accuracy_mnist(model, data_loader):\n",
    "    ## Toggle model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    ## Iterate through the dataset and perform inference for each sample.\n",
    "    ## Store inference results and target labels for AUC computation \n",
    "    with torch.no_grad():\n",
    "        #run through several batches, does inference for each and store inference results\n",
    "        # and store both target labels and inferenced scores\n",
    "        acc = 0.0\n",
    "        for image, target in data_loader:\n",
    "            image = image.cuda(); target = target.cuda()\n",
    "            probs = model(image)\n",
    "            preds = torch.argmax(probs, 1)\n",
    "            acc += torch.count_nonzero(preds == target)\n",
    "                \n",
    "        return acc/len(data_loader.dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to implement a convolutional neural network, named ``model_ex1``, with the following order and set-up:\n",
    "- Zero padding the image boundaries with 2 additional pixels\n",
    "- Convolution Layer 1 - kernel size = 5; output features = 6; padding = 0; stride = 1; containing bias\n",
    "- Relu Activation Function\n",
    "- Max Pooling Layer 1 - kernel size = 2; stride = 2\n",
    "- Convolution Layer 2 - kernel size = 5; output features = 16; padding = 0; stride = 1; containing bias\n",
    "- Relu Activation Function\n",
    "- Max Pooling Layer 2 - kernel size = 2; stride = 2\n",
    "- Convolution Layer 3 - kernel size = 5; output features = 120; padding = 0; stride = 1; containing bias\n",
    "- Relu Activation Function\n",
    "- Fully-connected Layer 1 - input features = 120; output features = 84; containing bias\n",
    "- Drop-out Layer - with dropout probability of 0.5\n",
    "- Relu Activation Function\n",
    "- Fully-connected Layer 2 - input features = 84; output features = 10; containing bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Implement a CNN, named model_ex1, with the architecture specified above #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement the training process for ``model_ex1`` using stochastic gradient descent algorithm to optimize the softmax cross-entropy loss function. Your best trained model must meet the following requirements:\n",
    "- Your best model should be named ``best_model_ex1``.\n",
    "- Obtaining at least 98.5\\% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Process #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we are going to evaluate your best trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inference stage for MNIST dataset #####\n",
    "test_acc = get_accuracy_mnist(best_model_ex1, test_loader_ex1)\n",
    "print('MNIST Test Accuracy: %0.3f%%' %(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - ChestXray14 Dataset (Total of 58 points)\n",
    ">### Brief explanation of the dataset\n",
    "> The ChestXray14 dataset contains more than 100,000 frontal chest x-rays and labels for 14 different conditions. These labels were extracted from radiologists' reports associated with each image using natural language processing techniques. It was released at the end of 2017 and was the largest publicly available x-rays dataset for developing deep learning models at the time. More information about the ChestXray14 dataset can be found here: https://stanfordmlgroup.github.io/projects/chexnet/. \n",
    "\n",
    "In this exercise, we will be using only a subset of this dataset, 14,999 images in total. The labels of this dataset follow a multi-label structure, which means that each image can have more than one label. First, we need to obtain the dataset. Please download the file **Data_Entry_2017_v2020.csv** from https://nihcc.app.box.com/v/ChestXray-NIHCC and **image_names_chestxray14.csv** from Canvas, and put them in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download part of the dataset if it was not downloaded yet and \n",
    "## put it to the xray14_dataset_folder folder\n",
    "\n",
    "##### Report download status #####\n",
    "def report_hook(count_so_far, block_size, total_size):\n",
    "    current_percentage = (count_so_far * block_size * 100 // total_size)\n",
    "    previous_percentage = ((count_so_far - 1) * block_size * 100 // total_size)\n",
    "    if current_percentage != previous_percentage:\n",
    "        sys.stdout.write('\\r' + str((count_so_far * block_size * 100 // total_size)) \\\n",
    "                         + '% of download completed')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "##### Download a subset of ChestXray14 dataset #####        \n",
    "if xray14_dataset_folder != '/scratch/tmp/deep_learning_datasets_ECE_6960_013/chestxray14':\n",
    "    os.makedirs(xray14_dataset_folder, exist_ok=True)\n",
    "    from urllib.request import urlretrieve\n",
    "    destination_file = xray14_dataset_folder + '/images_4.tar.gz'\n",
    "    link = 'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz'\n",
    "    if not os.path.isfile(destination_file):\n",
    "        urlretrieve(link, destination_file, reporthook = report_hook)\n",
    "\n",
    "    destination_file = xray14_dataset_folder + '/images_1.tar.gz'\n",
    "    link = 'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz'\n",
    "    if not os.path.isfile(destination_file):\n",
    "        urlretrieve(link, destination_file, reporthook = report_hook)        \n",
    "    \n",
    "##### Extract the downloaded file #####\n",
    "if xray14_dataset_folder != '/scratch/tmp/deep_learning_datasets_ECE_6960_013/chestxray14':\n",
    "    destination_file = xray14_dataset_folder + '/images_4.tar.gz'\n",
    "    tar = tarfile.open(destination_file, \"r:gz\")\n",
    "    tar.extractall(path = xray14_dataset_folder)\n",
    "    tar.close()\n",
    "    destination_file = xray14_dataset_folder + '/images_1.tar.gz'\n",
    "    tar = tarfile.open(destination_file, \"r:gz\")\n",
    "    tar.extractall(path = xray14_dataset_folder)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a Pytorch class structure for this dataset. The dataset class is used to load, index, and preprocess samples for training, validation, and testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chestxray14Dataset(Dataset):\n",
    "    ##### Initialize the class #####\n",
    "    def __init__(self, path_dataset_folder, split = 'train'):\n",
    "        ## Split parameter is used to specify which process the data is used for,\n",
    "        ## and it can be 'train', 'val', and 'test'\n",
    "        \n",
    "        self.path_image_folder = path_dataset_folder + '/images'\n",
    "        \n",
    "        ## Get the filenames of all images in the dataset\n",
    "        all_images_list = pd.read_csv('image_names_chestxray14.csv')\n",
    "\n",
    "        ## Read the labels file which needs to be placed in the same folder as this notebook\n",
    "        label_file = pd.read_csv('./Data_Entry_2017_v2020.csv')\n",
    "        \n",
    "        ## Merge labels and images information\n",
    "        examples_to_use = pd.merge(all_images_list, label_file)\n",
    "        \n",
    "        ## This is the name of all possible labels in this dataset.\n",
    "        ## The corresponding label of each sample is an array of 14 elements in which the elements are ordered\n",
    "        ## in the same way as \"self.set_of_finding_labels\" and the value of each element represents the \n",
    "        ## presence of that condition in the sample. For example, if \"cardiomegaly\" and \"pneumonia\" are the two \n",
    "        ## conditions presence a given sample, then the corresponding label of that sample is represented \n",
    "        ## by an array  - [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "        self.set_of_finding_labels = ['Atelectasis', 'Cardiomegaly','Effusion',  'Infiltration', 'Mass',\\\n",
    "                                      'Nodule', 'Pneumonia', 'Pneumothorax', 'Consolidation', 'Edema', \\\n",
    "                                      'Emphysema', 'Fibrosis','Pleural_Thickening', 'Hernia' ]\n",
    "        \n",
    "        ## Read labels from the label file\n",
    "        examples_to_use['Finding Labels'] = examples_to_use['Finding Labels'].str.split(pat = '|')\n",
    "        examples_to_use['Finding Labels'] = examples_to_use['Finding Labels'].apply(list).\\\n",
    "                                            to_frame(name='Finding Labels')\n",
    "        for finding_label in self.set_of_finding_labels:\n",
    "            examples_to_use[finding_label] = examples_to_use.apply(\\\n",
    "                                            lambda x: int(finding_label in x['Finding Labels']), axis=1)\n",
    "        \n",
    "        ## Get the list of all patient ids present in the dataset and split into\n",
    "        ## training, validation and testing by patient id, but not by list of examples\n",
    "        patient_ids = pd.unique(examples_to_use['Patient ID'])\n",
    "        patient_ids = pd.DataFrame(get_split(patient_ids, split), columns = ['Patient ID'])\n",
    "        \n",
    "        ## Filter the examples to only use the ones that have the chosen patient ids\n",
    "        examples_to_use = pd.merge(patient_ids,examples_to_use)        \n",
    "        \n",
    "        examples_to_use = examples_to_use[['Image Index'] + self.set_of_finding_labels]\n",
    "        self.image_list = examples_to_use['Image Index'].values\n",
    "        self.targets = examples_to_use[self.set_of_finding_labels].values\n",
    "        \n",
    "        ## Define data augmentation transformations for the input images. In this exercise, we use the following\n",
    "        ## transformations: square center cropping, resizing to 224x224 (to be similar as ImageNet dataset), \n",
    "        ## converting to tensor, normalizing per channel (i.e., R, G, and B) \n",
    "        ## with the average and standard deviation of images in the ImageNet dataset        \n",
    "        self.set_of_transforms = transforms.Compose(\n",
    "        [CropBiggestCenteredInscribedSquare(),\n",
    "         transforms.Resize(224),\n",
    "         transforms.ToTensor(), \n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "    ##### Retrieve a sample with the corresponding index #####\n",
    "    ## This function retrieve a sample from the dataset at the specified index \n",
    "    ## and returns an image and the corresponding label stored in Pytorch tensors     \n",
    "    def __getitem__(self, index):\n",
    "        curr_pil_image = Image.open(self.path_image_folder + '/' + self.image_list[index]).convert('RGB')\n",
    "        image_to_return = self.set_of_transforms(curr_pil_image)\n",
    "                \n",
    "        return image_to_return, torch.FloatTensor(self.targets[index])\n",
    "    \n",
    "    ##### Get the length of the dataset #####\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    ##### Access the name of conditions in the labels #####\n",
    "    def get_labels_name(self):\n",
    "        return self.set_of_finding_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then use the class function above to create structures for each training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets for this exercise\n",
    "train_dataset_ex2 = Chestxray14Dataset(xray14_dataset_folder)\n",
    "val_dataset_ex2 = Chestxray14Dataset(xray14_dataset_folder, split = 'val')\n",
    "test_dataset_ex2 = Chestxray14Dataset(xray14_dataset_folder, split = 'test')\n",
    "\n",
    "\n",
    "## Please contact the TA if any of the assertions fails\n",
    "assert(len(train_dataset_ex2) == 8837)\n",
    "assert(len(val_dataset_ex2) == 2924)\n",
    "assert(len(test_dataset_ex2) == 3238)\n",
    "assert(np.sum(train_dataset_ex2.targets)==5893)\n",
    "assert(np.sum(train_dataset_ex2.targets[:,7])==404)\n",
    "assert(np.sum(val_dataset_ex2.targets)==1810)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we report a statistical summary of the training set to get an idea of what this dataset looks like. We also show a sample of this dataset to get ourselves familiar with the type of images we are working with in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Helper function for display text below #####\n",
    "def join_str_array_to_labels(str_array,labels):\n",
    "    return ','.join(['\\n{}: {}'.format(labels[index_element], str_array_element) \n",
    "                for index_element, str_array_element in enumerate(str_array)])\n",
    "\n",
    "## Show the statistics of training set\n",
    "frequencies = np.sum(train_dataset_ex2.targets, axis = 0)/len(train_dataset_ex2)\n",
    "text_frequencies = ['{:.2f}%'.format(frequency*100) for frequency in frequencies]                    \n",
    "print('Percentage of positive examples in each class in the training set: ')\n",
    "print(join_str_array_to_labels(text_frequencies, train_dataset_ex2.get_labels_name()))\n",
    "\n",
    "## Plot a sample from the training set\n",
    "print('\\n\\nShowing one example from the dataset:')\n",
    "plt.imshow(train_dataset_ex2[1][0].cpu().numpy()[0,:,:], cmap = 'gray')\n",
    "print(join_str_array_to_labels(train_dataset_ex2[1][1],train_dataset_ex2.get_labels_name()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use AUC (also called AUROC) metric to evaluate the performance of a model. The AUC metric is defined as the area under the Receiver Operating Characteristic (ROC) curve, and has a value between 0 and 1. An AUC of 0.5 is what a model producing random outputs can maximally achieve. The higher the AUC is the better the model. In medical related applications, we want to avoid false negatives or false positives outcomes. Thus, the ROC curve is frequently used because it measures the trade-off between false positives and false negatives (sensitivity and specificity to be precise) of a model. Besides, ROC is insensitive to imbalanced datasets. We will define this metric in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate AUC metric #####\n",
    "## This function compute AUC from the given input arrays i.e., predicted value and ground truth arrays\n",
    "def auroc(logits_predicted, target):\n",
    "    fpr, tpr, _ = roc_curve(target, logits_predicted)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "##### Compute AUC of a given dataset #####\n",
    "## This function takes a model and Pytorch data loader as input. \n",
    "## The given model is used to predict the expected label for each sample in the Pytorch data loader. The \n",
    "## model output for each sample is an array with 14 elements corresponding with 14 conditions in the \n",
    "## ChestXray14 dataset. Then, the AUC is computed for each condition.\n",
    "def get_score_model_chestxray_binary_model(model, data_loader):\n",
    "    ## Toggle model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    ## Iterate through the dataset and perform inference for each sample.\n",
    "    ## Store inference results and target labels for AUC computation \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        logits_predicted = np.zeros([0, 14])\n",
    "        targets = np.zeros([0, 14])\n",
    "        ## Iterate through the dataset and perform inference for each sample.\n",
    "        ## Store inference results and target labels for AUC computation  \n",
    "        for image, target in data_loader:\n",
    "            image = image.cuda()\n",
    "            logit_predicted = model(image)\n",
    "            logits_predicted = np.concatenate((logits_predicted, logit_predicted.cpu().detach().numpy())\\\n",
    "                                              , axis = 0)\n",
    "            targets = np.concatenate((targets, target.cpu().detach().numpy()), axis = 0)\n",
    "            \n",
    "    ## Return a list of auc values in which each value corresponds to one of the 14 labels\n",
    "    return [auroc(logits_predicted[:,i], targets[:,i]) for i in range(14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 - Adopt an ImageNet Pretrained Model (22 points)\n",
    "In this section, you will adopt ResNet-18 pretrained on Imagenet dataset provided in the torchvision package from Pytorch library (https://pytorch.org/docs/stable/torchvision/models.html#id3) for ChestXray14 dataset. To obtain that goal, our tasks are as follows:\n",
    "- Setting up Pytorch dataloader for training, validation, and test sets\n",
    "- Modifying the ResNet-18 model to have 14 neurons, which corresponds to 14 labels, in the output layer\n",
    "- Selecting a loss function and justify your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Set-up Pytorch dataloader #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Modify ResNet-18 #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a short reasoning for your loss function selection:**\n",
    "This is a multi-label classification problem with binary class in each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define loss function #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement the training process for the modified ResNet-18 model. Your best trained model must achieve the mean AUC of 14 classes of at least 0.725 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Process #####\n",
    "### Your code starts here ###\n",
    "\n",
    "    \n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the desired accuracy on the validation set, test your best model on the test set, and specify the anomalies/labels of which your model achieves best and worst AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inference stage for ChestXray14 dataset #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2 - Implement Your Own Model (36 points) \n",
    "For this exercise, you are going to build your own model for ChestXray14 dataset. Your model has to satisfy the following conditions:\n",
    "- Containing at most 500,000 learnable parameters.\n",
    "- Training your model from scratch, i.e., none of the learnable parameters in your model is extracted from another pretrained model.\n",
    "- Obtaining at least 0.67 for the mean AUC of 14 classes on the validation set.\n",
    "\n",
    "In addition to having at least one model meets the requirements above, you need to show that you have tried:\n",
    "- at least 2 other architectures, i.e., with different number of layers, different number of feature maps at each layers, different combinations of layers, etc.\n",
    "- at least 2 hyperpameters with a set of values for each hyperparameter.\n",
    "\n",
    "Describe/Analyze the experiments you perform with plots, tables, and text on what works well and what does not. Please also include the code associated with these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Implement Your Own Model #####\n",
    "### Your code starts here ###\n",
    "\n",
    "    \n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will verify if your best model meets the number of learnable parameters requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Verify the number of learnable parameters requirement #####\n",
    "## ***** Please change the parameter inside the \"count_number_parameters\" \n",
    "##       to the name of the model you want to test *****\n",
    "if count_number_parameters(best_model_ex22) > 500000:\n",
    "    print('Warning! Your model exceeds the learnable parameters requirement!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your best model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Perform inference with your best model #####\n",
    "### Your code starts here \n",
    "\n",
    "\n",
    "### Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - RSNA Pneumonia Detection Challenge (Total of 30 points)\n",
    "\n",
    "Kaggle is a website that hosts machine learning competitions and datasets. It is a great resource for finding interesting projects for practicing deep learning concepts. In this exercise, we will be using one of the datasets available on Kaggle. Please follow the instructions below to synchronize this notebook with your Kaggle account to download the dataset that we are working with:\n",
    "1) Register an account with Kaggle if you haven't had one yet using this link - https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F\n",
    "\n",
    "2) Register for this Kaggle competition - https://www.kaggle.com/c/rsna-pneumonia-detection-challenge \n",
    "\n",
    "3) Select the account icon on the top right corner of the webpage\n",
    "\n",
    "4) Select \"My Account\"\n",
    "\n",
    "5) Select \"Create New API Token\" in the \"API\" module\n",
    "\n",
    "6) Save the json file where you can easily access it\n",
    "\n",
    "7) Execute the next two cells and enter the corresponding username and kaggle key located inside json file that you obtain from step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting Kaggle username for the API connection\n",
    "print(\"Write your kaggle username:\")\n",
    "kaggle_username = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting Kaggle API key for the API connection\n",
    "#check Exercise 2 for how to get this key\n",
    "print(\"Write your kaggle key:\")\n",
    "kaggle_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this exercise contains X-ray images in which each image is labeled with the diagnosis of pneumonia, and bounding boxes which are used to indicate the location evidence of pneumonia. The original dataset is used for an object detection task and evaluated with the mean average precision of bounding boxes (more details can be found here - https://www.kaggle.com/c/rsna-pneumonia-detection-challenge#evaluation). However, we are going to simplify this problem and convert the ground truth bounding boxes to a grid of binary labels. We then train a model using these modified labels, and use the AUC metric to evaluate them. \n",
    "\n",
    "**Note:** \n",
    " - Please download the file **image_names_kaggle_pneumonia.csv** provided with the assignment on Canvas and put it in the same directory as this notebook.\n",
    " - If you are using your own system, please also download this file (https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data?select=stage_2_train_labels.csv), unzip it, and put it in ``your_curr_dir/deep_learning_datasets_ECE_6960_013/kaggle_pneumonia/`` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if pneumonia_dataset_folder!='/scratch/tmp/deep_learning_datasets_ECE_6960_013/kaggle_pneumonia':\n",
    "    ## Download the dataset\n",
    "    os.environ['KAGGLE_USERNAME']=kaggle_username\n",
    "    os.environ['KAGGLE_KEY']=kaggle_key\n",
    "    os.makedirs(pneumonia_dataset_folder, exist_ok=True)\n",
    "    !kaggle competitions download -c rsna-pneumonia-detection-challenge -p $pneumonia_dataset_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pneumonia_dataset_folder!='/scratch/tmp/deep_learning_datasets_ECE_6960_013/kaggle_pneumonia':\n",
    "    ## Extract the dataset\n",
    "    c1 = pneumonia_dataset_folder+'/rsna-pneumonia-detection-challenge.zip'\n",
    "    !unzip -n $c1 -d $pneumonia_dataset_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a Pytorch class structure for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNAPneumoniaDetectionDataset(Dataset):\n",
    "        \n",
    "    ##### Compute grid label #####\n",
    "    ## This function receives a list of bounding boxes and transforms it \n",
    "    ## into a spatial grid of positive and negative labels where positive labels corresponding to\n",
    "    ## the bounding boxex locations\n",
    "    def get_grid(self, bounding_boxes):\n",
    "        ## The original size of each image - 1024x1024 \n",
    "        image_original_size = 1024\n",
    "                \n",
    "        grid_size = self.grid_size\n",
    "        \n",
    "        ## Start by creating a grid with the same size as the original image\n",
    "        ## since it is easy to know how each bounding box should be translated to 0/1 grid cells. Then,\n",
    "        ## set cells inside of the bounding boxes to 1, and cells outside of the bounding boxes to 0\n",
    "        this_grid = torch.zeros([1,1,image_original_size,image_original_size], dtype = torch.float)\n",
    "        for bounding_box in bounding_boxes:            \n",
    "            if bounding_box[0]!=bounding_box[0]:                \n",
    "                continue\n",
    "                \n",
    "            y1 = int(bounding_box[1])\n",
    "            y2 = y1 + int(bounding_box[3])\n",
    "            x1 = int(bounding_box[0])\n",
    "            x2 = x1 + int(bounding_box[2])\n",
    "            this_grid[:,:,y1:y2, x1:x2] = 1.0\n",
    "        \n",
    "        ## Reduce the image to a size that is a multiple of the grid size so that \n",
    "        ## average pooling can be used with the well defined kernel cells\n",
    "        first_resize_size = (image_original_size//self.grid_size)*self.grid_size\n",
    "        this_grid = torch.nn.functional.interpolate(this_grid, \\\n",
    "                                                    size = (first_resize_size, first_resize_size), \\\n",
    "                                                    mode = 'bilinear', align_corners = False)\n",
    "        this_grid = torch.nn.AvgPool2d(kernel_size = image_original_size//grid_size)(this_grid)        \n",
    "        ## this_grid should now have values between 0 to 1 specifying how much of that particular cell\n",
    "        ## is being occupied by a bounding box\n",
    "        \n",
    "        ## Set cells that have more than 50% of its area occupied by bounding boxes as positive labels\n",
    "        this_grid = ((this_grid[0,:,:,:][:]>0.5)*1.0).float()\n",
    "        return this_grid\n",
    "    \n",
    "\n",
    "    def __init__(self, path_dataset_folder, grid_size, split = 'train'):\n",
    "        self.path_image_folder = path_dataset_folder + '/stage_2_train_images'\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        ## Get the filenames of all images inside the dataset\n",
    "        all_images_list = !find $self.path_image_folder \\\n",
    "                            -type f -name \"*.dcm\" |sed 's#.*/##' | sed 's/\\.[^.]*$//'\n",
    "\n",
    "        ## Read the labels file\n",
    "        label_filename = path_dataset_folder + '/stage_2_train_labels.csv'\n",
    "        label_file = pd.read_csv(label_filename)\n",
    "        \n",
    "        ## Get the list of all patient ids present in the dataset, to split into\n",
    "        ## training, validation and testing by patient id\n",
    "        all_images_list =  pd.read_csv('image_names_kaggle_pneumonia.csv')['patientId'].values\n",
    "        all_images_list = pd.DataFrame(get_split(all_images_list, split), columns = ['patientId'])\n",
    "        examples_to_use = pd.merge(all_images_list, label_file)\n",
    "        \n",
    "        ## Put all bounding boxes coordinates as a list of coordinates in a single column    \n",
    "        dataframe_with_listed_bounding_boxes = examples_to_use\n",
    "        dataframe_with_listed_bounding_boxes['x'] = \\\n",
    "                                        dataframe_with_listed_bounding_boxes['x'].apply(lambda x: [x])\n",
    "        dataframe_with_listed_bounding_boxes['y'] = \\\n",
    "                                        dataframe_with_listed_bounding_boxes['y'].apply(lambda x: [x])\n",
    "        dataframe_with_listed_bounding_boxes['width'] = \\\n",
    "                                        dataframe_with_listed_bounding_boxes['width'].apply(lambda x: [x])\n",
    "        dataframe_with_listed_bounding_boxes['height'] = \\\n",
    "                                        dataframe_with_listed_bounding_boxes['height'].apply(lambda x: [x])\n",
    "        dataframe_with_listed_bounding_boxes['bounding_boxes'] = \\\n",
    "                                        dataframe_with_listed_bounding_boxes['x'] + \\\n",
    "                                        dataframe_with_listed_bounding_boxes['y'] + \\\n",
    "                                        dataframe_with_listed_bounding_boxes['width'] + \\\n",
    "                                        dataframe_with_listed_bounding_boxes['height']\n",
    "        dataframe_with_listed_bounding_boxes = \\\n",
    "                                        dataframe_with_listed_bounding_boxes[['patientId', 'bounding_boxes']]\n",
    "        \n",
    "        ## Since bounding boxes for the same patient id are stored in more than one label line, \n",
    "        ## group them by patient id and create a list of bounding boxes for each patient id\n",
    "        dataframe_with_listed_bounding_boxes = dataframe_with_listed_bounding_boxes.groupby('patientId')\n",
    "        dataframe_with_listed_bounding_boxes = dataframe_with_listed_bounding_boxes.\\\n",
    "                                                    aggregate(lambda x: tuple(x)).reset_index()\n",
    "        \n",
    "        ## Transform the lists of bounding boxes to grids\n",
    "        dataframe_with_listed_bounding_boxes['bounding_boxes'] = \\\n",
    "                        dataframe_with_listed_bounding_boxes['bounding_boxes'].apply(lambda x: self.get_grid(x))\n",
    "        \n",
    "        ## Join the tables with targets and grids\n",
    "        examples_to_use = examples_to_use[['patientId', 'Target']].drop_duplicates()\n",
    "        examples_to_use = pd.merge(examples_to_use, dataframe_with_listed_bounding_boxes)\n",
    "        assert(len(examples_to_use) == len(all_images_list))\n",
    "        \n",
    "        self.image_list = examples_to_use['patientId'].values\n",
    "        self.targets = examples_to_use['Target'].values\n",
    "        self.grids = examples_to_use['bounding_boxes'].values        \n",
    "        \n",
    "        ## Define data augmentation transformations for the input images. For this dataset, we use the following\n",
    "        ## transformations: square center cropping, resizing to 224x224 (to be similar as ImageNet dataset),\n",
    "        ## converting to grayscale, converting to tensor, normalizing per channel (i.e., R, G, and B) \n",
    "        ## with the average and standard deviation of images in the ImageNet dataset    \n",
    "        self.set_of_transforms = transforms.Compose(\n",
    "                                    [CropBiggestCenteredInscribedSquare(),\n",
    "                                    transforms.Resize(224),\n",
    "                                    transforms.Grayscale(num_output_channels=3),\n",
    "                                    transforms.ToTensor(), \n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "    ##### Retrieve a sample with the corresponding index #####\n",
    "    ## This function retrieve a sample from the dataset at the specified index \n",
    "    ## and returns a 3-channel image, single binary label specifying the target label,\n",
    "    ## and 1-channel spatial grid\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image_to_return = self.set_of_transforms(Image.fromarray(\\\n",
    "                                            pydicom.dcmread(self.path_image_folder + '/' \\\n",
    "                                                            + self.image_list[index] + '.dcm').pixel_array))\n",
    "        label = torch.FloatTensor(self.targets[index:index + 1])\n",
    "        grid_label = torch.FloatTensor(self.grids[index]) \n",
    "        return image_to_return, label, grid_label\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the class function above to create structures for each training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets for this exercise. This takes a few minutes (~10 mins on CADE) to complete\n",
    "train_dataset_ex3 = RSNAPneumoniaDetectionDataset(pneumonia_dataset_folder, grid_size  = 14)\n",
    "val_dataset_ex3 = RSNAPneumoniaDetectionDataset(pneumonia_dataset_folder, split = 'val', grid_size  = 14)\n",
    "test_dataset_ex3 = RSNAPneumoniaDetectionDataset(pneumonia_dataset_folder, split = 'test', grid_size  = 14)\n",
    "\n",
    "## Please contact the TA if any of the assertions fails\n",
    "assert(len(train_dataset_ex3) == 16010)\n",
    "assert(len(val_dataset_ex3) == 5337)\n",
    "assert(len(test_dataset_ex3) == 5337)\n",
    "assert(sum(train_dataset_ex3.targets) == 3603)\n",
    "assert(sum(val_dataset_ex3.targets) == 1195)\n",
    "assert(np.sum([grid.numpy() for grid in train_dataset_ex3.grids]) == 81251)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the summary of the training set and visualize a few samples to get ourselves familiar with the type of images and labels we are working with in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frequency = np.sum(train_dataset_ex3.targets, axis = 0)/len(train_dataset_ex3)                \n",
    "print('Percentage of positive examples for whole images: ' + '{:.2f}'.format(frequency*100) + '%')\n",
    "\n",
    "\n",
    "frequency = np.sum([grid.numpy() for grid in train_dataset_ex3.grids])/len(train_dataset_ex3)/14/14\n",
    "print('Percentage of positive examples for grid cells: ' + '{:.2f}'.format(frequency*100) + '%')\n",
    "\n",
    "def imresize(arr, size, resample):\n",
    "    return np.array(Image.fromarray(arr).resize(size, resample))\n",
    "\n",
    "def plot_grid_over_xray(example):\n",
    "    image = example[0].numpy()[0,:,:]\n",
    "    print('Label pneumonia: ' + str(example[1][0].cpu().numpy()))\n",
    "    max1 = np.max(image); min1 = np.min(image)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow((image-min1)/(max1 - min1), cmap = 'gray')\n",
    "    ax.imshow(imresize(example[2].numpy()[0,:,:], (224, 224), resample  = Image.NEAREST)\\\n",
    "                                                  , cmap='jet', alpha=0.3, resample = True)\n",
    "    plt.show()\n",
    "\n",
    "print('\\nVisualizing a few examples: ')\n",
    "## Plot a few images from the dataset\n",
    "## The red areas correspond to ones label in the grid (pneumonia presence)\n",
    "## The blue areas correspond to zeros label in the grid (pneumonia absence)\n",
    "example =  train_dataset_ex3[1]\n",
    "plot_grid_over_xray(train_dataset_ex3[1])\n",
    "plot_grid_over_xray(train_dataset_ex3[10])\n",
    "plot_grid_over_xray(train_dataset_ex3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the AUC metric used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute AUC for location prediction #####\n",
    "## This function takes a model and Pytorch data loader as input. \n",
    "## The given model is used to predict the expected label for each sample in the Pytorch data loader. \n",
    "## The predicted value and label for each sample are the grids and each cell in the grid is considered as \n",
    "## a different example. Hence, the AUC is computed over all possible grid cells in all the samples.\n",
    "def get_score_model_pneumonia_location_model(model, data_loader):\n",
    "    ## Toggle model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    ## Turn off gradients since they will not be used here and this is to make the inference faster\n",
    "    with torch.no_grad():\n",
    "        logits_predicted = np.zeros([0, 1])\n",
    "        targets = np.zeros([0,1])\n",
    "                \n",
    "        for image, target, grid in data_loader:\n",
    "            image = image.cuda()\n",
    "            logit_predicted = model(image)  \n",
    "            \n",
    "            ## Each grid cell target is considered a different example for calculating the score\n",
    "            ## for that, all outputs and target are reshaped to have only one value in the second dimension\n",
    "            logits_predicted = np.concatenate((logits_predicted, \\\n",
    "                                               logit_predicted.view([-1,1]).cpu().numpy()), axis = 0)\n",
    "            targets = np.concatenate((targets, grid.view([-1,1]).cpu().numpy()), axis = 0)\n",
    "    return auroc(logits_predicted[:,0], targets[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1 - Simplify Detection Problem (26 points) \n",
    "In this section, you are going to modify the ResNet-18 model pretrained on ImageNet from PyTorch to output a flattened 14x14 grid. Your tasks involves:\n",
    "- Setting up Pytorch dataloader for training, validation, and test sets\n",
    "- Modifying the ResNet-18 model as specified above\n",
    "- Training the modified model\n",
    "- Obtaining an AUC of at least 0.97 on the validation set\n",
    "- Visualizing a few test examples that have corrected locations prediction and a few incorrect ones. Remember to show the predicted and ground truth for each example.\n",
    "\n",
    "Hint:\n",
    "the output of layer3 in the ResNet-18 model is already a 14x14 output, so you just need to remove layer4 from the model and use the output of layer3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Set-up Pytorch dataloader #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code starts here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Modify ResNet-18 #####\n",
    "## Implement the modified ResNet-18 model as specified above\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Training Process #####\n",
    "### Your code starts here ###\n",
    "\n",
    "    \n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training is done, we are going to test your best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Inference state for pneumonia dataset #####\n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to visualize a few success and failure prediction examples. The failure case is defined in this context when the model completely predicts the wrong label, e.g., a sample doesn't have pneumonia presence which means that sample doesn't contain any bounding box label; however, the model predicts otherwise.\n",
    "- Success cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing success cases \n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Failure Cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Visualizing failure cases \n",
    "### Your code starts here ###\n",
    "\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2 (4 points)\n",
    "In this last section, we are going to use the grid output produced by the model from Exercise 3.1 to diagnose pneumonia condition. Each cell in the grid output produced by the model above indicates the probability of that cell contains signs of the presence of pneumonia. Thus, the probability of a sample with pneumonia presence is indicated by the maximum probability of the grid output of that sample. Your task is to complete the function ``pneumonia_diagnosis_with_location_model`` that takes 2 parameters, ``model`` and ``data_loader``, as input. For each sample in ``data_loader``, you will compute the probability of that sample has pneumonia presence using the grid output produced by the ``model``. This function then returns a numpy array, ``logits_predicted``, containing the probability of pneumonia presence for all the samples inside ``data_loader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pneumonia_diagnosis_with_location_model(model, data_loader):\n",
    "    ### Your code starts here ###\n",
    "    \n",
    "    \n",
    "    ### Your code ends here ###\n",
    "    return logits_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Compute AUC score for the new metric defined in previous cell #####\n",
    "def get_score_model_pneumonia_binary_with_location_model(model, data_loader):               \n",
    "    logits_predicted = pneumonia_diagnosis_with_location_model(model, data_loader)    \n",
    "    targets = np.zeros([0,1])\n",
    "    for image, target, grid in data_loader:\n",
    "        image = image.cuda()                                    \n",
    "        targets = np.concatenate((targets, target.cpu().numpy()), axis = 0)\n",
    "            \n",
    "    return auroc(logits_predicted[:,0], targets[:,0])\n",
    "\n",
    "##### Evaluate model from Exercise 3.1 with the new metric defined in the previous cell #####    \n",
    "auc_test = get_score_model_pneumonia_binary_with_location_model(best_model_ex31, test_loader_ex31)\n",
    "print('AUC test binary of Exercise 2.3 model: %0.5f' %(np.mean(auc_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
