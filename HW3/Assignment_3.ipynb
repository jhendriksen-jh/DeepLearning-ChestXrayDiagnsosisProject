{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 (100 points)\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots and texts. You might need to create new cells with original code or text for your analyses. This assignment has a total of 100 points.\n",
    "\n",
    "For assignment submission, you will submit this notebook file (.ipynb) on Canvas with cells executed and outputs visible. Your submitted notebook **must** follow these guidelines:\n",
    "- No other dataset than the provided datasets should be used.\n",
    "- Training, validation and testing splits should be the same as the ones provided.\n",
    "- The cell outputs in your delivered notebook should be reproducible.\n",
    "- Please print out the evaluation metric evidence that your model achieves the evaluation requirement. Optionally, you can also add plot of how the evaluation metric changes over the course of training process.\n",
    "- Please provide code associated with the conclusions you make in your analysis as well as code that is used to generate plots, images, etc. for your analysis.\n",
    "- All code must be your own work. Code cannot be copied from external sources or another students. You may copy code from cells that are pre-defined in this notebook if you think it is useful to reuse in another question.\n",
    "- All images must be generated from data in your code. Do **NOT** import/display images that are generated outside your code.\n",
    "- Your analyses must be your own, but if you quote text or equations from another source please make sure to cite the appropriate references.\n",
    "- Your input with code will be marked with comments ``###your code starts here###`` and ``###your code ends here###`` to specify where you need to write your code. You can also create a new code cell in between those marked comments.\n",
    "\n",
    "\n",
    "**NOTES:**\n",
    "- PyTorch needs to be downloaded and installed properly.\n",
    "- You should use PyTorch 1.7 or later.\n",
    "- If you need to import a different package than the ones already imported, please check with the TA if you can do so.\n",
    "- Cells should be run in order, using Shift+Enter.\n",
    "- Read all the provided code cells and comments as they contain variables and information that you may need to use to complete the notebook.\n",
    "- To create a new text cell, select \"+\" button on the menu bar and change its type from \"Code\" to \"Markdown\".\n",
    "- To modify a text cell, double click on it.\n",
    "- More details on how to format markdown text can be found here: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "- Your home directory on CADE machines has a small disk quota. It might be necessary, depending on how much your home directory is already occupied, to store the virtual environment inside a folder in ```/scratch/tmp/```. \n",
    "\n",
    "**Tips for training deep learning models:**\n",
    "- We assume a GPU of at least 4GB of memory is available. If you want to try running the assignment with a GPU that has less than that, you can try changing the argument passed when calling the ```define_gpu_to_use``` function.  If you are getting out-of-memory errors for the GPU, you may want to check what is occupying the GPU memory by using the command ```!nvidia-smi```, which gives a GPU's usage report. However, if you are using your own Windows machine, the nvidia-smi command used in the define_gpu_to_use function will not work. You can skip running this function but please check to make sure your GPU has a sufficient amount of free memory.\n",
    "- Here are a few PyTorch details not to forget:\n",
    "    - Toggle train/eval mode for your model\n",
    "    - Reset the gradients with ```zero_grad()``` before each call to ```backward()```\n",
    "    - Check if the loss you are using receives logits or probabilities, and adapt your model output accordingly.\n",
    "    - Reinstantiate your model every time you are starting a new training so that the weights are reset, if you plan to reuse the variable name.\n",
    "    - Pass the model's parameters to the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0 - Setting-up Infrastructure (Total of 0 points)\n",
    "## + Installing Libraries:\n",
    "Follow the steps below to install a few more additional libraries:\n",
    "- Open another terminal and activate your virtual environment ``source your_virtualvenv_name/bin/activate``.\n",
    "- Install the following libraries via pip using these commands: ``pip install scikit-image``, ``pip install scikit-learn``, ``pip install imageio``, and ``pip install imagecodecs``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import tarfile\n",
    "import imageio\n",
    "import imagecodecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import zipfile\n",
    "import collections\n",
    "from skimage import morphology\n",
    "from skimage.measure import block_reduce\n",
    "import scipy\n",
    "from torch.utils.data import Dataset\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from packaging import version\n",
    "\n",
    "##### Checking Torch library requirement #####\n",
    "my_torch_version = torch.__version__\n",
    "minimum_torch_version = '1.7'\n",
    "if version.parse(my_torch_version) < version.parse(minimum_torch_version):\n",
    "    print('Warning!!! Your Torch version %s does NOT meet the minimum requirement!\\\n",
    "            Please update your Torch library\\n' %my_torch_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Requesting GPU Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Checking the System #####\n",
    "try:\n",
    "    hostname = !hostname\n",
    "    if 'lab' in hostname[0] and '.eng.utah.edu' in hostname[0]:\n",
    "        IN_CADE = True\n",
    "    else:\n",
    "        IN_CADE = False\n",
    "except:\n",
    "    IN_CADE = False\n",
    "    \n",
    "##### Requesting a GPU #####\n",
    "## This function locates an available gpu for usage. In addition, this function reserves a specificed\n",
    "## memory space exclusively for your account. The memory reservation prevents the decrement in computational\n",
    "## speed when other users try to allocate memory on the same gpu in the shared systems, i.e., CADE machines. \n",
    "## Note: If you use your own system which has a GPU with less than 4GB of memory, remember to change the \n",
    "## specified mimimum memory.\n",
    "def define_gpu_to_use(minimum_memory_mb = 3500):    \n",
    "    thres_memory = 600 #\n",
    "    gpu_to_use = None\n",
    "    try: \n",
    "        os.environ['CUDA_VISIBLE_DEVICES']\n",
    "        print('GPU already assigned before: ' + str(os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    for i in range(16):\n",
    "        free_memory = !nvidia-smi --query-gpu=memory.free -i $i --format=csv,nounits,noheader\n",
    "        if free_memory[0] == 'No devices were found':\n",
    "            break\n",
    "        free_memory = int(free_memory[0])\n",
    "        \n",
    "        if free_memory>minimum_memory_mb-thres_memory:\n",
    "            gpu_to_use = i\n",
    "            break\n",
    "            \n",
    "    if gpu_to_use is None:\n",
    "        print('Could not find any GPU available with the required free memory of ' + str(minimum_memory_mb) \\\n",
    "              + 'MB. Please use a different system for this assignment.')\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_to_use)\n",
    "        print('Chosen GPU: ' + str(gpu_to_use))\n",
    "        x = torch.rand((256,1024,minimum_memory_mb-thres_memory)).cuda()\n",
    "        x = torch.rand((1,1)).cuda()        \n",
    "        del x\n",
    "        \n",
    "## Request a gpu and reserve the memory space\n",
    "define_gpu_to_use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - DRIVE and STARE Dataset (Total of 100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to implement a segmentation model to extract blood vessels in retinal digital images. We will utilize two well-known retina image datasets, DRIVE (https://www.isi.uu.nl/Research/Databases/DRIVE/) and STARE (http://cecas.clemson.edu/~ahoover/stare/). First, we need to download these datasets. Please follow the steps below to download the images and then put them in the same directory as this notebook:\n",
    " - Register an account with DRIVE dataset - https://grand-challenge.org/accounts/login/?next=https%3A//drive.grand-challenge.org/participants/registration/create/\n",
    " - Download ``datasets.zip`` file at https://drive.grand-challenge.org/Download/\n",
    " - Download ``stare-images.tar`` from http://cecas.clemson.edu/~ahoover/stare/probing/stare-images.tar\n",
    " - Download ``labels-vk.tar`` from http://cecas.clemson.edu/~ahoover/stare/probing/labels-vk.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define a few functions to extract images from the downloaded file for DRIVE and STARE datasets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Removing Small Regions #####\n",
    "## This function removes small regions (<size) of a given binary image.\n",
    "def remove_small_regions(img, size):\n",
    "    img = morphology.remove_small_objects(img, size)\n",
    "    img = morphology.remove_small_holes(img, size)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Resizing Image #####\n",
    "## This function resizes a given input image to be half of the original size.\n",
    "def resize_img(img):\n",
    "    if len(img.shape)==3:\n",
    "        img = np.array(Image.fromarray(img).resize(((img.shape[1]+1)//2,(img.shape[0]+1)//2), \\\n",
    "                                                   PIL.Image.BILINEAR))\n",
    "    else:\n",
    "        img = block_reduce(img, block_size=(2, 2), func=np.max)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DRIVE Dataset Loading Function #####\n",
    "## This function unzips the downloaded file. Then, it loads the corresponding mask \n",
    "## for each image in the DRIVE dataset.\n",
    "def drive_read_images(filetype, dest_folder):\n",
    "    zip_ref = zipfile.ZipFile('datasets.zip', 'r')\n",
    "    zip_ref.extractall('datasets/drive')\n",
    "    zip_ref.close()\n",
    "    zip_ref = zipfile.ZipFile('datasets/drive/training.zip', 'r')\n",
    "    zip_ref.extractall('datasets/drive')\n",
    "    zip_ref.close()\n",
    "\n",
    "    all_images = []\n",
    "    for item in sorted(os.listdir(dest_folder)):\n",
    "        if item.endswith(filetype):            \n",
    "            img = imageio.imread(dest_folder + item)\n",
    "            if len(img.shape) == 3:\n",
    "                img = np.pad(img , ((12,12), (69,70),(0,0)), mode = 'constant')\n",
    "            else:\n",
    "                img = np.pad(img , ((12,12), (69,70)), mode = 'constant')\n",
    "            img = resize_img(img)\n",
    "            img = img/255.\n",
    "            img = img.astype(np.float32)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img.astype(np.float32)\n",
    "                img = np.expand_dims(img, axis = 2)\n",
    "            all_images.append(img)\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### STARE Dataset Loading Function #####\n",
    "## This function untars the downloaded file. Then, it loads the corresponding mask \n",
    "## for each image in the STARE dataset.\n",
    "def stare_read_images(tar_filename, dest_folder, do_mask = False):\n",
    "    tar = tarfile.open(tar_filename)\n",
    "    tar.extractall(dest_folder)\n",
    "    tar.close()\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "    for item in sorted(os.listdir(dest_folder)):\n",
    "        if item.endswith('gz'):\n",
    "            with gzip.open(dest_folder + item, 'rb') as f_in:\n",
    "                with open(dest_folder + item[:-3], 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(dest_folder + item) \n",
    "            img = imageio.imread(dest_folder + item[:-3])\n",
    "            if len(img.shape) == 3:\n",
    "                img = np.pad(img , ((1,2), (2,2),(0,0)), mode = 'constant')\n",
    "            else:\n",
    "                img = np.pad(img , ((1,2), (2,2)), mode = 'constant')\n",
    "            img = resize_img(img)\n",
    "            img = img/255.\n",
    "            img = img.astype(np.float32)\n",
    "            if len(img.shape) == 2:\n",
    "                img = img.astype(np.float32)\n",
    "                img = np.expand_dims(img, axis = 2)\n",
    "            all_images.append(img)\n",
    "            if do_mask:\n",
    "                mask = (1-remove_small_regions(np.prod((img<50/255.)*1.0, axis = 2)>0.5, 1000))*1.0\n",
    "                mask = np.expand_dims(mask, axis = 2)\n",
    "                all_masks.append(mask.astype(np.float32))\n",
    "    if do_mask:\n",
    "        return all_images, all_masks\n",
    "    else:\n",
    "        return all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DRIVE dataset provides its own split for training and testing. However, the mask images of the test split in DRIVE dataset are currently missing. Thus, we are going to use our own train/validation/test splits with the provided function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Spliting a dataset for training, validatation, and testing #####\n",
    "## This function splits a given dataset into 3 subsets of 70%-10%-20% for train-val-test, \n",
    "## respectively, and is used internally in the dataset classes below.\n",
    "def get_split(array_to_split, split):\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(array_to_split)\n",
    "    np.random.seed()\n",
    "    if split == 'train':\n",
    "        array_to_split = array_to_split[:int(len(array_to_split)*0.7)]\n",
    "    elif split == 'val':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.7):int(len(array_to_split)*0.8)]\n",
    "    elif split == 'test':\n",
    "        array_to_split = array_to_split[int(len(array_to_split)*0.8):]\n",
    "    return array_to_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 - Data Augmentation (12 points)\n",
    "Data augmentation is an essential step in training deep models. Using data augmentation usually helps to improve the performance of a deep model. Please write a short reasoning why data augmentation helps to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will be using horizontal and vertical flipping. Your task is first to provide a short reasoning why horizontal and vertical flipping is suitable for this dataset. Second, please list at least one other data augmentation method that could also be used for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement the horizontal and vertical flipping transformations by filling out the missing codes in the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Defining Transformations #####\n",
    "## The transformations below will be applied to input image, segmentation ground-truth and mask.\n",
    "\n",
    "## Applying transformations to all array in list x \n",
    "def _iterate_transforms(transform, x):\n",
    "    for i, xi in enumerate(x):\n",
    "        x[i] = transform(x[i])\n",
    "    return x\n",
    "\n",
    "## Redefining Pytorch composed transform so that it uses the _iterate_transforms function\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for transform in self.transforms:\n",
    "            x = _iterate_transforms(transform, x) \n",
    "        return x\n",
    "\n",
    "## Generating randomize odd for vertical flipping class\n",
    "class RandomVerticalFlipGenerator(object):\n",
    "    def __call__(self, img):\n",
    "        self.random_n = random.uniform(0, 1)\n",
    "        return img\n",
    "\n",
    "## Performing vertical flip using randomization provided by generator class above\n",
    "class RandomVerticalFlip(object):\n",
    "    def __init__(self, gen):\n",
    "        self.gen = gen\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.gen.random_n < 0.5:\n",
    "            ### Your code starts here ###\n",
    "            \n",
    "            ### Your code ends here ###\n",
    "            return flipped_img\n",
    "        return img\n",
    "\n",
    "## Generating randomize odd for horizontal flipping class  \n",
    "class RandomHorizontalFlipGenerator(object):\n",
    "    def __call__(self, img):\n",
    "        self.random_n = random.uniform(0, 1)\n",
    "        return img\n",
    "\n",
    "## Performing horizontal flip using randomization provided by generator class above\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, gen):\n",
    "        self.gen = gen\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.gen.random_n < 0.5:\n",
    "            ### Your code starts here ###\n",
    "            \n",
    "            ### Your code ends here ###\n",
    "            return flipped_img\n",
    "        return img "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a Pytorch class structure for this dataset. The dataset class is used to load, index, and preprocess samples for training, validation, and testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaDataset(Dataset):    \n",
    "    ##### Initializing the class #####\n",
    "    def __init__(self, retina_array, split = 'train', do_transform=False):\n",
    "        ## Split parameter is used to specify which process the data is used for,\n",
    "        ## and it can be 'train', 'val', and 'test'\n",
    "        indexes_this_split = get_split(np.arange(len(retina_array), dtype = np.int), split)\n",
    "        self.retina_array = [self.transpose_first_index(retina_array[i]) for i in indexes_this_split]\n",
    "        self.split = split\n",
    "        self.do_transform = do_transform\n",
    "        \n",
    "    ##### Retrieving a sample with the corresponding index #####\n",
    "    ## This function retrieve a sample from the dataset at the specified index \n",
    "    ## and returns a list in which the first element is input image to be segmented,\n",
    "    ## the second element is the segmentation ground truth, and the last element is the mask of image region\n",
    "    def __getitem__(self, index):\n",
    "        sample = [torch.FloatTensor(x) for x in self.retina_array[index]]\n",
    "        if self.do_transform:\n",
    "            v_gen = RandomVerticalFlipGenerator()\n",
    "            h_gen = RandomHorizontalFlipGenerator()\n",
    "            t = Compose([\n",
    "                v_gen,\n",
    "                RandomVerticalFlip(gen=v_gen),\n",
    "                h_gen,\n",
    "                RandomHorizontalFlip(gen=h_gen),\n",
    "            ])\n",
    "            sample = t(sample)\n",
    "        return sample\n",
    "    \n",
    "    ##### Accessing the length of the dataset #####\n",
    "    def __len__(self):\n",
    "        return len(self.retina_array)\n",
    "    \n",
    "    ##### Flipping the third dimension #####\n",
    "    def transpose_first_index(self, x):\n",
    "        x2 = (np.transpose(x[0], [2,0,1]), np.transpose(x[1], [2,0,1]), np.transpose(x[2], [2,0,1]))\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we utilize the class function above to create structures for each training, validation, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading Data #####\n",
    "## This function loads DRIVE and STARE datasets into a list of list of arrays. \n",
    "## The first element in the list is the list of input images, the second element is a list of segmentation\n",
    "## ground truth, and the last element is a list of masks of image region.\n",
    "## The original images were padded as squares so that we can fit them to a traditional CNN. \n",
    "## The masks are binary images, and contain the location of the original image (labeled as 1) and \n",
    "## the padded region (labeled as 0). These masks are used to limit where outputs are backpropagated \n",
    "## for trained and which region of the image should be used for scoring. \n",
    "def get_retina_array():\n",
    "    stare_images, stare_mask = stare_read_images(\"stare-images.tar\", 'datasets/stare/images/', do_mask = True)  \n",
    "    stare_segmentation = stare_read_images(\"labels-vk.tar\", 'datasets/stare/segmentations/')   \n",
    "    \n",
    "    drive_training_images = drive_read_images('tif', 'datasets/drive/training/images/')\n",
    "    drive_training_segmentation = drive_read_images('gif', 'datasets/drive/training/1st_manual/')\n",
    "    drive_training_mask = drive_read_images('gif', 'datasets/drive/training/mask/')\n",
    "    \n",
    "    return list(zip(stare_images+drive_training_images,#+drive_test_images, \n",
    "                           stare_segmentation+drive_training_segmentation,#+drive_test_segmentation, \n",
    "                           stare_mask + drive_training_mask))# + drive_test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_array = get_retina_array()\n",
    "\n",
    "train_dataset = RetinaDataset(retina_array, do_transform = True)\n",
    "val_dataset = RetinaDataset(retina_array, split = 'val')\n",
    "test_dataset = RetinaDataset(retina_array, split = 'test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visulize a few samples to get ourselves familiar with the type of images we are working with in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualing a few cases in the training set\n",
    "for batch_idx, (data, segmentation, mask) in enumerate( RetinaDataset(retina_array)):\n",
    "    if batch_idx%15 == 0: \n",
    "        plt.figure()\n",
    "        plt.title(\"Input Image\")\n",
    "        plt.imshow(data[:,:,:].permute([1,2,0]).cpu().numpy())\n",
    "        plt.figure()\n",
    "        plt.title(\"Segmentation ground truth\")\n",
    "        plt.imshow(segmentation[0,:,:].cpu().numpy())\n",
    "        plt.figure()\n",
    "        plt.title(\"Mask\")\n",
    "        plt.imshow(mask[0,:,:].cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use F1 score to evaluate the segmentation results. The F1 score is computed from precision and recall, and has value between 0 (the worst possible value) and 1 (the best possible value). More information about F1 score can be found here - https://en.wikipedia.org/wiki/F-score. For this exercise, we consider the foreground as positive class and the background as negative class in computing F1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculating F1 metric #####\n",
    "def get_score_model(model, data_loader):\n",
    "    ## Toggling model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    ## Turning off gradients\n",
    "    with torch.no_grad():\n",
    "        logits_predicted = np.zeros([0, 1, 304, 352])\n",
    "        segmentations = np.zeros([0, 1, 304, 352])\n",
    "        \n",
    "        mean_f1 = 0.0\n",
    "        ## Iterating through the dataset and perform inference for each sample.\n",
    "        ## Then, the F1 score is computed for each sample. \n",
    "        for image, segmentation, mask  in data_loader:\n",
    "            image = image.cuda()\n",
    "            logit_predicted = model(image)\n",
    "            logit_predicted = logit_predicted.cpu().detach().numpy()*mask.numpy()\n",
    "            segmentation = segmentation.numpy()*mask.numpy()\n",
    "            \n",
    "            ## Computing F1 score for each sample in the batch\n",
    "            for i in range(segmentation.shape[0]):\n",
    "                curr_seg = segmentation[i,...].reshape([-1])\n",
    "                curr_logit = logit_predicted[i,...].reshape([-1]) > 0   \n",
    "                curr_f1 = f1_score(curr_seg, curr_logit)\n",
    "                mean_f1 += curr_f1\n",
    "                \n",
    "    ## Returning the mean F1 of the entire dataset\n",
    "    return mean_f1/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 - Computing Weight Vector (7 points)\n",
    "In many real world applications, the dataset tends to be imbalanced. It is also the case for this retina image dataset. In this dataset, there are more background samples than there are foreground samples. In this exercise, your task is to examine the imbalanced between foreground and background by providing the following analyses:\n",
    "- The percentage of positive label (foreground) in this dataset.\n",
    "- The ratio of negative label (background) to positive label (foreground). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The percentage of positive label (foreground): **Write your answer here**.\n",
    "- The ratio of negative label (background) to positive label (foreground): **Write your answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to use the ratio of negative label to positive label as weight vector for the loss function. In the code cell below, please provide an implementation of a weighted loss function for a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3 - Implementing U-Net (55 points)\n",
    "In this section, you are going to implement a well-known segmentation model, called U-Net. Your implementation should follow the architecture as described in the paper (https://arxiv.org/pdf/1505.04597.pdf - Fig. 1 and Section 2) with a few modifications below:\n",
    "- The input should have 3 channels, and the output should have only one channel (binary output).\n",
    "- Adding 2D batch normalization layer between convolution layer and Relu transformation, i.e., changing CONV->RELU to CONV->BN->RELU.\n",
    "- Padding the convolution layers so that the outputs of the convolution layers have the same spatial size as the inputs. With this modification, the cropping operation before the concatenation in the skip connection can be removed.\n",
    "- Upsampling operation should be implemented with the torch.nn.ConvTransposed2D layer. More details to understand what they meant in the paper can be found in the video here (https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) starting at 2:22.\n",
    "- Reducing the number of channels of **ALL** internal layers 4 times. For example, the number of channels in the first convolution layer in the paper is 64. The first convolution layer in your implementation should have 16 channels instead. \n",
    "- No need to implement the initialization of weights as described in the paper. The default weight initialization from Pytorch is sufficient. \n",
    "- Your U-Net implementation should be named ``model_ex13`` and the best model should be called ``best_model_ex13``.\n",
    "- Your implementation of U-Net should achieve an F1 score of at least 0.75 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, your task is to complete the ``training_stage`` function below. You must use the provided masks when computing the loss. In other words, you must only compute the loss for the pixels that have values of 1 in the corresponding mask images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_stage(epoch, optimizer, loss, model, train_loader, val_loader):\n",
    "    model.train()\n",
    "    loss_values = []\n",
    "    for batch_idx, (data, segmentation, mask) in enumerate(train_loader):\n",
    "        ### Your code starts here ###\n",
    "        \n",
    "        ### Your code ends here ###\n",
    "        loss_values.append(loss_value.item())\n",
    "    return np.mean(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_ex13.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "n_epochs = 200\n",
    "\n",
    "## Using the scheduler module to reduce the learning rate after reaching a plateau. \n",
    "## More information about the scheduler can be found at \n",
    "## https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "current_best_score = -1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ## Train the model\n",
    "    loss_value = training_stage(epoch, optimizer, loss, model_ex13, train_loader, val_loader)\n",
    "    ## Evaluate the current model\n",
    "    f1_val = get_score_model(model_ex13, val_loader)\n",
    "    f1_train = get_score_model(model_ex13, train_loader)\n",
    "    current_score = f1_val\n",
    "    ## Save the model\n",
    "    if current_score > current_best_score:\n",
    "        current_best_score = current_score\n",
    "        best_model_ex13 = copy.deepcopy(model_ex13)\n",
    "    print('Train Epoch: {:d} \\tLoss: {:.5f}'.format(epoch,loss_value))\n",
    "    print('F1 train: {:.5f} \\t F1 val: {:.5f}'.format(f1_train, f1_val))\n",
    "    ## Activate scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score of test set: {:.5f}'.format(get_score_model(best_model_ex13, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4 - Visualizing Output (8 points)\n",
    "In this section, please plot the predicted output of a few samples from the validation set as well as the corresponding ground-truth. In addition, please provide a short analysis on the type of mistakes that you are able to distinguish. For example, where in the image does the model wrongly identify as foreground? where in the image does the model wrongly identfy as background?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your analysis here**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.5 - Implementing U-Net Without Skip Connections (18 points)\n",
    "In this section, your task is to implement a similar U-Net model, called ``model_ex15``, as specificed in Exercise 1.3 with the exception that this model does **NOT** contain any skip connections. To compensate for not having the extra channels coming from the skip connection, you need to double the number of output channels in the upsampling layer. \n",
    "\n",
    "**Notes:**\n",
    "- Your best model should be named ``best_model_ex15``.\n",
    "- Your implementation of this variant should achieve an F1 score of at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the implemented model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_ex15.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "n_epochs = 200\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "current_best_score = -1\n",
    "for epoch in range(n_epochs):\n",
    "    ## Train the model\n",
    "    loss_value = training_stage(epoch, optimizer, loss, model_ex15, train_loader, val_loader)\n",
    "    f1_val = get_score_model(model_ex15, val_loader)\n",
    "    f1_train = get_score_model(model_ex15, train_loader)\n",
    "    current_score = f1_val\n",
    "    ## Save the model\n",
    "    if current_score > current_best_score:        \n",
    "        current_best_score = current_score\n",
    "        best_model_ex15 = copy.deepcopy(model_ex15)\n",
    "    print('Train Epoch: {:d} \\tLoss: {:.5f}'.format(epoch,loss_value))\n",
    "    print('F1 train: {:.5f} \\t F1 val: {:.5f}'.format(f1_train, f1_val))\n",
    "    ## Activate scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score of test set: {:.5f}'.format(get_score_model(best_model_ex15, test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please provide a similar visualization as Exercise 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code starts here ###\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen qualitatively from the visualization section and quantitatively from the F1 score, the U-Net model without skip connections performs poorly specifically in the thin section of the foreground. In the cell below, please provide a short reasonsing of 1) which causes the model to performance poorly in the thin section of the foreground? 2) Why the skip connections help to overcome that limitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
